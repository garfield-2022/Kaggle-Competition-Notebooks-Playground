{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"},{"sourceId":6478229,"sourceType":"datasetVersion","datasetId":3742543}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Regression of Used Car Prices. Playground Series - Season 4, Episode 9. Private Score: 63313.29307. Public Score: 72411.72359.","metadata":{}},{"cell_type":"markdown","source":"This notebook builds three simple models: xgb, lgbm and cat. Hyperparameters are fine-tuned for each of them. Optimized blending weights are found through Optuna tuning. A cv score of 72584 is reached on full train dataset. Main references include:\n\n1. Used Car Price - Analytics & Predictions by Usman Bashir, https://www.kaggle.com/code/usmanbashir1/used-car-price-analytics-predictions .\n2. How to Remove Outliers for Machine Learning, https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/ .\n3. S4E9 Exotics Identification? No & Proof by Todd Gardiner, https://www.kaggle.com/code/toddgardiner/s4e9-exotics-identification-no-proof/notebook .\n4. [AutoML GrandPrix] 3rd Place Solution Write Up by lash_fire, https://github.com/Dhanush-M555/AutoML_GrandPrix_2024_My_Solutions/blob/main/%5BAutoML%20GrandPrix%5D%203rd%20Place%20Solution.ipynb .\n5. Residual Analysis in Regression, https://stattrek.com/regression/residual-analysis .\n6. Stacking: XGB + LGBM + CATB + ANN by rÄ±za temizel, https://www.kaggle.com/code/rzatemizel/stacking-xgb-lgbm-catb-ann#Voting-vs-Stacking .\n7. PS4E5| Simple Blending + Detailed Explanation by Trupolog Helper, https://www.kaggle.com/code/trupologhelper/ps4e5-simple-blending-detailed-explanation .\n8. Visualize - Price vs. Model_Year and Price vs. Milage by Chris Deotte, https://www.kaggle.com/competitions/playground-series-s4e9/discussion/532952","metadata":{}},{"cell_type":"code","source":"#%load_ext cudf.pandas\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\nimport matplotlib.pyplot as plt\n\nimport re\nimport sklearn\nfrom sklearn import preprocessing\nfrom scipy.stats import chi2_contingency\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom numpy import percentile\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import HuberRegressor\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, Pool\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.base import clone\nfrom sklearn.metrics import mean_squared_error\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport optuna\nfrom optuna.visualization import plot_param_importances\nfrom optuna.samplers import RandomSampler, TPESampler, CmaEsSampler\nfrom optuna.pruners import HyperbandPruner\nfrom functools import partial\n\n# print('The scikit-learn version is {}.'.format(sklearn.__version__))","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:32:45.288244Z","iopub.execute_input":"2024-09-25T12:32:45.288547Z","iopub.status.idle":"2024-09-25T12:32:51.810152Z","shell.execute_reply.started":"2024-09-25T12:32:45.288515Z","shell.execute_reply":"2024-09-25T12:32:51.809116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install cmaes","metadata":{"execution":{"iopub.status.busy":"2024-09-23T15:08:29.852579Z","iopub.execute_input":"2024-09-23T15:08:29.853769Z","iopub.status.idle":"2024-09-23T15:08:47.384928Z","shell.execute_reply.started":"2024-09-23T15:08:29.853719Z","shell.execute_reply":"2024-09-23T15:08:47.383481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_seed = 0\nn_fold = 5","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:32:55.485184Z","iopub.execute_input":"2024-09-25T12:32:55.486423Z","iopub.status.idle":"2024-09-25T12:32:55.491264Z","shell.execute_reply.started":"2024-09-25T12:32:55.486368Z","shell.execute_reply":"2024-09-25T12:32:55.490110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Import and Glance at the Data.","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/playground-series-s4e9/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s4e9/test.csv\")\noriginal = pd.read_csv(\"/kaggle/input/used-car-price-prediction-dataset/used_cars.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:32:56.923833Z","iopub.execute_input":"2024-09-25T12:32:56.924209Z","iopub.status.idle":"2024-09-25T12:32:58.354172Z","shell.execute_reply.started":"2024-09-25T12:32:56.924175Z","shell.execute_reply":"2024-09-25T12:32:58.353230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Train dataset have 188533 entries, and test dataset have 125690 entries. Three columns \"fuel_type\", \"accident\", \"clean_title\" have null values in both train and test datasets. Numerical features include \"model_year\" and \"milage\". All the other feaures are categorical: \"brand\", \"model\", \"fuel_type\", \"engine\", \"transmission\", \"ext_col\", \"int_col\", \"accident\" and \"clean_title\". Target variable is \"price\". \n\nThe original dataset have 4009 samples. Only \"model_year\" column is numerical. All the others are categorical. The same three columns \"fuel_type\", \"accident\" and \"clean_title\" have null values.\n\n.info(), .describe(), .isnull().sum() and .unique() are frequently used throughout the notebook to check intermediate results.","metadata":{}},{"cell_type":"code","source":"train.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:32:59.005080Z","iopub.execute_input":"2024-09-25T12:32:59.005447Z","iopub.status.idle":"2024-09-25T12:32:59.053609Z","shell.execute_reply.started":"2024-09-25T12:32:59.005415Z","shell.execute_reply":"2024-09-25T12:32:59.052561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Feature Engineering.","metadata":{}},{"cell_type":"markdown","source":"Change formats of both milage and price columns in the original datasets. New features: car age, horsepower, engine_displacement, no_of_cylinder, transmission_speed and transmission_type. Drop engine, transmission and model_year columns.","metadata":{}},{"cell_type":"code","source":"original['milage'] = original['milage'].str.replace(',', '').str.replace(' mi.', '').astype(float)\noriginal['price'] = original['price'].str.replace(',', '').str.replace('$', '').astype(float)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:01.171272Z","iopub.execute_input":"2024-09-25T12:33:01.172389Z","iopub.status.idle":"2024-09-25T12:33:01.190280Z","shell.execute_reply.started":"2024-09-25T12:33:01.172336Z","shell.execute_reply":"2024-09-25T12:33:01.189090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def engine_feat_extract(data):\n    df=data.copy()\n    \n    df['horsepower'] = df['engine'].str.extract(r'(\\d+\\.\\d+)HP')\n    \n    df['engine'] = df['engine'].apply(lambda x: x.replace(' Litre', 'L'))\n    df['engine'] = df['engine'].apply(lambda x: x.replace(' Liter', 'L'))\n    df['engine'] = df['engine'].apply(lambda x: x.replace(' L', 'L'))\n    df['engine_displacement'] = df['engine'].str.extract(r'(\\d+\\.\\d+)L')\n    \n    df['engine'] = df['engine'].apply(lambda x: x.replace('V-', 'V'))\n    df['no_of_cylinder'] = df['engine'].str.extract(r'( \\d+ | V\\d+ | I\\d+ | W\\d+ | H\\d+ |I\\d+ |V\\d+ |V\\d+|I\\d+)')\n    df['no_of_cylinder'] = df['no_of_cylinder'].str.strip()\n    df['no_of_cylinder'] = df['no_of_cylinder'].str.replace('V','').str.replace('I','').str.replace('H','').str.replace('W','')\n    \n    df.drop('engine', axis=1, inplace=True)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:02.920182Z","iopub.execute_input":"2024-09-25T12:33:02.921059Z","iopub.status.idle":"2024-09-25T12:33:02.933068Z","shell.execute_reply.started":"2024-09-25T12:33:02.921006Z","shell.execute_reply":"2024-09-25T12:33:02.931726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transmission_feat_extract(data):\n    df=data.copy()\n    \n    df['transmission'] = df['transmission'].str.replace('Automatic','AT').str.replace('A/T','AT').str.replace('At','AT')\n    df['transmission'] = df['transmission'].str.replace('M/T','MT').str.replace('Mt','MT').str.replace('Manual', 'MT')\n    df['transmission'] = df['transmission'].str.replace('Auto', 'AT')\n    df['transmission'] = df['transmission'].str.replace('-Spd', '-Speed').str.replace('-SPEED', '-Speed')\n    df['transmission'] = df['transmission'].str.replace('Single-Speed', '1-Speed').str.replace(' Speed', '-Speed')\n    \n    df['transmission_speed'] = df['transmission'].str.extract(r'(\\d+)-Speed')\n    df['transmission_speed'] = df['transmission_speed'].str.strip()\n    \n    df['transmission_type'] = df['transmission']\n    df.loc[df.transmission.str.contains('AT/MT', na=False, case=False), 'transmission_type'] = 'AT/MT'\n    df.loc[df.transmission.str.contains('AT', na=False, case=False), 'transmission_type'] = 'AT'\n    df.loc[df.transmission.str.contains('MT', na=False, case=False), 'transmission_type'] = 'MT'\n    df['transmission_type'] = df['transmission_type'].str.replace('1-Speed Fixed Gear', 'Fixed Gear')\n    df.loc[df.transmission_type.str.contains('6-Speed', na=False, case=False), 'transmission_type'] = np.nan\n    df.loc[df.transmission_type.str.contains('7-Speed', na=False, case=False), 'transmission_type'] = np.nan\n    \n    #df['transmission_type'] = df['transmission'].str.replace(r'([0-9]+-Speed)', '', regex=True)\n    \n    df.drop('transmission', axis=1, inplace=True)\n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:05.062458Z","iopub.execute_input":"2024-09-25T12:33:05.063067Z","iopub.status.idle":"2024-09-25T12:33:05.080171Z","shell.execute_reply.started":"2024-09-25T12:33:05.063017Z","shell.execute_reply":"2024-09-25T12:33:05.078935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineering(data):\n    df=data.copy()\n    \n    df = engine_feat_extract(df)\n    df = transmission_feat_extract(df)\n    \n    df['age'] = 2025 - df['model_year']\n    df.drop(['model_year'], axis=1, inplace=True)\n    \n    df['milage_per_year'] = df['milage'] / df['age']\n    \n    df['brand'] = df['brand'].str.lower()\n    df['model'] = df['model'].str.lower()\n    df['ext_col'] = df['ext_col'].str.lower()\n    df['int_col'] = df['int_col'].str.lower()\n    df['transmission_type'] = df['transmission_type'].str.lower()\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:07.291038Z","iopub.execute_input":"2024-09-25T12:33:07.291945Z","iopub.status.idle":"2024-09-25T12:33:07.300781Z","shell.execute_reply.started":"2024-09-25T12:33:07.291879Z","shell.execute_reply":"2024-09-25T12:33:07.299604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original = feature_engineering(original)\ntrain = feature_engineering(train)\ntest = feature_engineering(test)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:09.863635Z","iopub.execute_input":"2024-09-25T12:33:09.864245Z","iopub.status.idle":"2024-09-25T12:33:18.017929Z","shell.execute_reply.started":"2024-09-25T12:33:09.864205Z","shell.execute_reply":"2024-09-25T12:33:18.016810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now all the datasets have 15 features. ","metadata":{}},{"cell_type":"markdown","source":"# 3. Check Outliers. ","metadata":{}},{"cell_type":"markdown","source":"The minimum and maximum car ages on all the three datasets are 1 and 51. Data look normal. No need to remove any samples.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3)\nsns.boxplot(data=original[['age']], ax=axes[0]).set_title('Original')\nsns.boxplot(data=train[['age']],ax=axes[1]).set_title('Train')\nsns.boxplot(data=test[['age']],ax=axes[2]).set_title('Test')\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The minimum and maximum milages on three datasets are 100 and 405000. Data look normal. No need to remove any samples.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3)\nsns.boxplot(data=original[['milage']],ax=axes[0]).set_title('Original')\nsns.boxplot(data=train[['milage']],ax=axes[1]).set_title('Train')\nsns.boxplot(data=test[['milage']],ax=axes[2]).set_title('Test')\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The three highest prices in both original and train datasets are 2954083,1950995 and 1599000. All the other prices are below 1,000,000. Because the test dataset may contain those outliers as well, we can not remove them. Keep all the samples for now.","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2)\nsns.boxplot(data=original[['price']],ax=axes[0]).set_title('Original')\nsns.boxplot(data=train[['price']],ax=axes[1]).set_title('Train')\nfig.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fig, axes = plt.subplots(1, 2)\n#sns.histplot(data=original['price'], kde=True, ax=axes[0]).set_title('Original')\n#sns.histplot(data=original['price'], kde=True, ax=axes[1]).set_title('Train')\n#fig.tight_layout()\n#plt.show()\n\n#data = original['price']\n#q25, q75 = percentile(data, 25), percentile(data, 75)\n#iqr = q75 - q25\n#cut_off = iqr * 1.5\n#lower, upper = q25 - cut_off, q75 + cut_off\n#original = original[(data >= lower) & (data <= upper)].reset_index()\n#original.drop('index', axis=1, inplace=True)\n\n#pca = PCA()\n#pca.fit(X_original)\n#pd.DataFrame(pca.transform(X_original))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Fill in Missing Values.","metadata":{}},{"cell_type":"markdown","source":"Need to deal with both numerical('horsepower', 'engine_displacement', 'no_of_cylinder' and 'transmission_speed') and categorical('fuel_type', 'accident', 'clean_title' and 'transmission_type') missing values. ","metadata":{}},{"cell_type":"code","source":"#fig, axes = plt.subplots(1, 3)\n#sns.boxplot(data=original[['horsepower']],ax=axes[0]).set_title('Original')\n#sns.boxplot(data=train[['horsepower']],ax=axes[1]).set_title('Train')\n#sns.boxplot(data=test[['horsepower']],ax=axes[2]).set_title('Test')\n#fig.tight_layout()\n#plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OrdinalEncoder() with encoded_missing_value set to -1 will turn missing values to -1. Compared to SimpleImputer and KNNImputer, I think OrdinalEncoder() makes sense for this dataset. Turn other categorical variables into numbers.","metadata":{}},{"cell_type":"code","source":"def fill_missing(data):\n    df = data.copy()\n    \n    for col in ['horsepower', 'engine_displacement', 'no_of_cylinder', 'transmission_speed']:\n        df[[col]] = df[[col]].fillna(df[col].mode()[0])\n        df[[col]] = df[[col]].astype(float)\n                \n    for col in ['fuel_type','accident','clean_title','transmission_type']:\n        enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\n        df[[col]] = enc.fit_transform(df[[col]])    \n        \n    #df['fuel_type'] = df['fuel_type'].fillna('none')\n    #df['accident'] = df['accident'].fillna('empty')\n    #df['clean_title'] = df['clean_title'].fillna('empty')    \n        \n    for col in ['brand','model','ext_col','int_col']:\n        enc = preprocessing.OrdinalEncoder()\n        df[[col]] = enc.fit_transform(df[[col]])            \n        \n    return df    ","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:18.019843Z","iopub.execute_input":"2024-09-25T12:33:18.020244Z","iopub.status.idle":"2024-09-25T12:33:18.030799Z","shell.execute_reply.started":"2024-09-25T12:33:18.020199Z","shell.execute_reply":"2024-09-25T12:33:18.029832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"original = fill_missing(original)\ntrain = fill_missing(train)\ntest = fill_missing(test)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:19.464931Z","iopub.execute_input":"2024-09-25T12:33:19.465461Z","iopub.status.idle":"2024-09-25T12:33:21.397099Z","shell.execute_reply.started":"2024-09-25T12:33:19.465408Z","shell.execute_reply":"2024-09-25T12:33:21.396048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Relationships among Features.","metadata":{}},{"cell_type":"markdown","source":"Now we have a full value dataset. We are ready to explore many kinds of relationships hidden in the dataset. ","metadata":{}},{"cell_type":"code","source":"#original.corr()\n#train.corr()\n#original.hist()\n#train.hist()\n#test.hist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 6. Build Models.","metadata":{}},{"cell_type":"code","source":"X_train = train.drop(['price'], axis=1)\ny_train = train['price']\n\nX_original = original.drop(['price'], axis=1)\ny_original = original['price']\n\nX_test = test","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:22.140457Z","iopub.execute_input":"2024-09-25T12:33:22.140862Z","iopub.status.idle":"2024-09-25T12:33:22.156805Z","shell.execute_reply.started":"2024-09-25T12:33:22.140825Z","shell.execute_reply":"2024-09-25T12:33:22.156069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\nscaler.fit(pd.concat([X_train, X_original], axis=0))","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:24.765626Z","iopub.execute_input":"2024-09-25T12:33:24.766238Z","iopub.status.idle":"2024-09-25T12:33:24.813822Z","shell.execute_reply.started":"2024-09-25T12:33:24.766196Z","shell.execute_reply":"2024-09-25T12:33:24.812819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-09-25T12:33:27.015976Z","iopub.execute_input":"2024-09-25T12:33:27.016387Z","iopub.status.idle":"2024-09-25T12:33:27.036010Z","shell.execute_reply.started":"2024-09-25T12:33:27.016348Z","shell.execute_reply":"2024-09-25T12:33:27.034956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Three models are built. Hyperparameters are tuned by Optuna for each of them.","metadata":{}},{"cell_type":"markdown","source":"*  6.1 XGB","metadata":{}},{"cell_type":"code","source":"#fig = plot_param_importances(study)\n#fig.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T16:36:52.040256Z","iopub.execute_input":"2024-09-23T16:36:52.040656Z","iopub.status.idle":"2024-09-23T16:36:52.044606Z","shell.execute_reply.started":"2024-09-23T16:36:52.040619Z","shell.execute_reply":"2024-09-23T16:36:52.043735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'eta': 0.02,\n        'n_estimators': 10000,\n        'alpha': trial.suggest_float('alpha', 0., 1.0),\n        'lambda': trial.suggest_float('lambda', 1., 100.0),\n        'subsample': trial.suggest_float('subsample', 0., 1.0), #column-wise sampling \n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0., 1.0), #row-wise sampling\n        'max_depth': trial.suggest_int('max_depth', 3, 20),\n        'min_child_weight': trial.suggest_float(\"min_child_weight\", 1., 50.),\n        'gamma': trial.suggest_float('gamma', 0., 1.0),\n        #'max_bin': trial.suggest_int('max_bin', 20, 400), # Gpu does not accept customized max_bin.\n        'tree_method': 'gpu_hist',\n        'eval_metric': 'rmse',\n        'random_state': random_seed,\n        'objective': 'reg:squarederror',\n        'booster': 'gbtree',\n        'grow_policy': 'lossguide',\n        'verbosity': 1,\n        'device': 'gpu',\n    }\n    \n    cv = KFold(n_fold, shuffle=True, random_state=random_seed)\n    cv_splits = cv.split(X_train, y_train)\n    \n    val_preds = np.zeros(len(X_train))\n\n    model = XGBRegressor(**params)\n    \n    for train_idx, val_idx in cv_splits:\n        X_train_fold = pd.concat([X_train.iloc[train_idx], X_original], axis=0)\n        y_train_fold = pd.concat([y_train.iloc[train_idx], y_original], axis=0)\n        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train.iloc[val_idx]\n    \n        X_train_fold = scaler.transform(X_train_fold)\n        X_val_fold = scaler.transform(X_val_fold)\n        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=100, verbose=False)\n                \n        val_preds[val_idx] = model.predict(X_val_fold)\n        gc.collect()\n        \n        rmse_full = mean_squared_error(y_train, val_preds, squared=False)\n    \n    return rmse_full","metadata":{"execution":{"iopub.status.busy":"2024-09-23T17:11:57.170338Z","iopub.execute_input":"2024-09-23T17:11:57.171178Z","iopub.status.idle":"2024-09-23T17:11:57.181962Z","shell.execute_reply.started":"2024-09-23T17:11:57.171137Z","shell.execute_reply":"2024-09-23T17:11:57.180927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_name = \"xgb\"\nsampler = TPESampler(multivariate=True, group=True, seed=random_seed)\n\noptimize = False\nif optimize:\n    study = optuna.create_study(study_name=study_name, sampler=sampler, direction=\"minimize\", load_if_exists=True)\n    study.optimize(objective, timeout=3600*4)\n    \n    print(f\"best optimized rmse: {study.best_value:0.5f}\") #72612\n    print(f\"best hyperparameters: {study.best_params}\") \n    xgb_params = study.best_params\nelse:\n    xgb_params = {\n        'eta': 0.02,\n        'n_estimators': 10000,\n        'alpha': 0.40415346051079754, \n        'lambda': 99.36388385514442, \n        'subsample': 0.9323744584461396, \n        'colsample_bytree': 0.41058416064357234, \n        'max_depth': 11, \n        'min_child_weight': 33.5644386196058, \n        'gamma': 0.2796042692041842,\n        'tree_method': 'gpu_hist',\n        'eval_metric': 'rmse',\n        'random_state': random_seed,\n        'objective': 'reg:squarederror',\n        'booster': 'gbtree',\n        'grow_policy': 'lossguide',\n        'verbosity': 1,\n        'device': 'gpu',\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:14:53.176846Z","iopub.execute_input":"2024-09-23T21:14:53.177507Z","iopub.status.idle":"2024-09-23T21:14:53.186616Z","shell.execute_reply.started":"2024-09-23T21:14:53.177466Z","shell.execute_reply":"2024-09-23T21:14:53.185632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 6.2 LGBM","metadata":{}},{"cell_type":"code","source":"#fig = plot_param_importances(study)\n#fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'learning_rate': 0.01,\n        'n_estimators': 2000,\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'min_child_samples': trial.suggest_int('min_child_samples', 20, 500),\n        'max_depth': trial.suggest_int('max_depth', 2, 13),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-9, 1.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-9, 1.0, log=True),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0), #row-wise sampling\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0), #column-wise sampling   \n        'min_gain_to_split': trial.suggest_float('min_gain_to_split', 0.1, 1.0),\n        'bin_construct_sample_cnt': trial.suggest_int('bin_construct_sample_cnt', 20000, 300000),\n        #'max_bin': trial.suggest_int('max_bin', 20, 400), # Gpu does not accept customized max_bin.\n        'random_state': random_seed,\n        'verbosity': -1,\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'device': 'gpu',\n        'eval_metric': 'l2',\n    }\n    \n    cv = KFold(n_fold, shuffle=True, random_state=random_seed)\n    cv_splits = cv.split(X_train, y_train)\n    \n    val_preds = np.zeros(len(X_train))\n    \n    model = LGBMRegressor(**params)\n    \n    for train_idx, val_idx in cv_splits:\n        X_train_fold = pd.concat([X_train.iloc[train_idx], X_original], axis=0)\n        y_train_fold = pd.concat([y_train.iloc[train_idx], y_original], axis=0)\n        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train.iloc[val_idx]\n    \n        X_train_fold = scaler.transform(X_train_fold)\n        X_val_fold = scaler.transform(X_val_fold)\n        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], eval_metric='l2', callbacks=[lgb.early_stopping(stopping_rounds=100)])\n                \n        val_preds[val_idx] = model.predict(X_val_fold)\n        gc.collect()\n        \n        rmse_full = mean_squared_error(y_train, val_preds, squared=False)\n    \n    return rmse_full","metadata":{"execution":{"iopub.status.busy":"2024-09-24T14:39:02.839939Z","iopub.execute_input":"2024-09-24T14:39:02.840827Z","iopub.status.idle":"2024-09-24T14:39:02.852237Z","shell.execute_reply.started":"2024-09-24T14:39:02.840782Z","shell.execute_reply":"2024-09-24T14:39:02.851357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#sqlite_db = \"sqlite:///lgbm.db\"\n# storage=sqlite_db, \nstudy_name = \"lgbm\"\nsampler = TPESampler(multivariate=True, group=True, seed=random_seed)\n\noptimize = False\nif optimize:\n    study = optuna.create_study(study_name=study_name, sampler=sampler, direction=\"minimize\", load_if_exists=True)\n    study.optimize(objective, timeout=3600*4)\n    \n    print(f\"best optimized rmse: {study.best_value:0.5f}\") \n    print(f\"best hyperparameters: {study.best_params}\") \n    lgbm_params = study.best_params\nelse:\n    lgbm_params = {\n        'learning_rate': 0.01,\n        'n_estimators': 2000,\n        'num_leaves': 237, \n        'min_child_samples': 241, \n        'max_depth': 12, \n        'reg_alpha': 1.8869868007564816e-07, \n        'reg_lambda': 9.206863421105129e-06, \n        'colsample_bytree': 0.541469428945828, \n        'subsample': 0.6290239247477459, \n        'min_gain_to_split': 0.36460105927641795, \n        'bin_construct_sample_cnt': 119286,\n        'random_state': random_seed,\n        'verbosity': -1,\n        'objective': 'regression',\n        'boosting_type': 'gbdt',\n        'device': 'gpu',\n        'eval_metric': 'l2',\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-24T18:52:42.664633Z","iopub.execute_input":"2024-09-24T18:52:42.665298Z","iopub.status.idle":"2024-09-24T18:52:42.674896Z","shell.execute_reply.started":"2024-09-24T18:52:42.665255Z","shell.execute_reply":"2024-09-24T18:52:42.673937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* 6.3 Cat","metadata":{}},{"cell_type":"code","source":"#fig = plot_param_importances(study)\n#fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        'learning_rate': 0.05,\n        'iterations': 2000,\n        'depth': trial.suggest_int('depth', 4, 16),\n        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 100),\n        #\"rsm\": trial.suggest_float(\"rsm\", 0.5, 1.0),\n        'random_strength': trial.suggest_float(\"random_strength\", 0., 10.),\n        'border_count': trial.suggest_int('border_count', 20, 500),\n        'bagging_temperature': trial.suggest_float(\"bagging_temperature\", 0., 1.),\n        'random_state': random_seed,\n        'grow_policy': 'SymmetricTree',\n        'loss_function': 'RMSE',\n        'eval_metric': 'RMSE',\n        'task_type': 'GPU',\n        'logging_level': 'Silent',\n    }\n    \n    cv = KFold(n_fold, shuffle=True, random_state=random_seed)\n    cv_splits = cv.split(X_train, y_train)\n    \n    val_preds = np.zeros(len(X_train))\n    \n    model = CatBoostRegressor(**params)\n    \n    for train_idx, val_idx in cv_splits:\n        X_train_fold = pd.concat([X_train.iloc[train_idx], X_original], axis=0)\n        y_train_fold = pd.concat([y_train.iloc[train_idx], y_original], axis=0)\n        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train.iloc[val_idx]\n    \n        X_train_fold = scaler.transform(X_train_fold)\n        X_val_fold = scaler.transform(X_val_fold)\n        \n        #X_train_fold_pool = Pool(X_train_fold, y_train_fold, cat_features=X_train_fold.columns.values)\n        #X_val_fold_pool = Pool(X_val_fold, y_val_fold, cat_features=X_val_fold.columns.values)\n\n        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=100, verbose=False)\n                \n        val_preds[val_idx] = model.predict(X_val_fold)\n        gc.collect()\n        \n        rmse_full = mean_squared_error(y_train, val_preds, squared=False)\n    \n    return rmse_full","metadata":{"execution":{"iopub.status.busy":"2024-09-25T14:43:06.756890Z","iopub.execute_input":"2024-09-25T14:43:06.757525Z","iopub.status.idle":"2024-09-25T14:43:06.768022Z","shell.execute_reply.started":"2024-09-25T14:43:06.757487Z","shell.execute_reply":"2024-09-25T14:43:06.766946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study_name = \"cat\"\nsampler = TPESampler(multivariate=True, group=True, seed=random_seed)\n\noptimize = False\nif optimize:\n    study = optuna.create_study(study_name=study_name, sampler=sampler, direction=\"minimize\", load_if_exists=True)\n    study.optimize(objective, timeout=3600*3)\n    \n    print(f\"best optimized rmse: {study.best_value:0.5f}\") \n    print(f\"best hyperparameters: {study.best_params}\") \n    cat_params = study.best_params\nelse:\n    cat_params = {\n        'learning_rate': 0.05,\n        'iterations': 2000,\n        'depth': 9, \n        'l2_leaf_reg': 12.85233671225663, \n        'random_strength': 7.379142037012981, \n        'border_count': 301, \n        'bagging_temperature': 0.23617322853025788,\n        'random_state': random_seed,\n        'grow_policy': 'SymmetricTree',\n        'loss_function': 'RMSE',\n        'eval_metric': 'RMSE',\n        'task_type': 'GPU',\n        'logging_level': 'Silent',\n    }","metadata":{"execution":{"iopub.status.busy":"2024-09-25T17:48:46.902749Z","iopub.execute_input":"2024-09-25T17:48:46.903618Z","iopub.status.idle":"2024-09-25T17:48:46.911811Z","shell.execute_reply.started":"2024-09-25T17:48:46.903579Z","shell.execute_reply":"2024-09-25T17:48:46.910912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 7. Cross Validation and Prediction Analysis.","metadata":{}},{"cell_type":"markdown","source":"Apply three hyperparameter-tuned models on the train dataset to get the cv scores and on the test dataset to make predictions.","metadata":{}},{"cell_type":"code","source":"def validation(model):\n    cv = KFold(n_fold, shuffle=True, random_state=random_seed)\n    cv_splits = cv.split(X_train, y_train)\n    \n    #val_errors = list()\n    test_preds = np.zeros(len(X_test))\n    val_preds = np.zeros(len(X_train))\n    \n    for train_idx, val_idx in cv_splits:\n        X_train_fold = pd.concat([X_train.iloc[train_idx], X_original], axis=0)\n        y_train_fold = pd.concat([y_train.iloc[train_idx], y_original], axis=0)\n        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train.iloc[val_idx]\n        \n        X_train_fold = scaler.transform(X_train_fold)\n        X_val_fold = scaler.transform(X_val_fold)\n        \n        #X_train_fold_pool = Pool(X_train_fold, y_train_fold, cat_features=X_train_fold.columns.values)\n        #X_val_fold_pool = Pool(X_val_fold, y_val_fold, cat_features=X_val_fold.columns.values)\n        \n        model_cloned = clone(model)\n        \n        if isinstance(model_cloned, XGBRegressor):\n            model_cloned.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=100, verbose=False)\n        elif isinstance(model_cloned, LGBMRegressor):\n            model_cloned.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], eval_metric='l2', callbacks=[lgb.early_stopping(stopping_rounds=100)])\n        elif isinstance(model_cloned, CatBoostRegressor):\n            model_cloned.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=100, verbose=False)\n        \n        val_preds[val_idx] = model_cloned.predict(X_val_fold)\n        \n        #y_val_pred = model_cloned.predict(X_val_fold)\n        #val_error= mean_squared_error(y_val_fold, y_val_pred, squared=False)\n        #val_errors.append(val_error)\n        \n        y_pred = model_cloned.predict(X_test)\n        test_preds += y_pred / n_fold\n        #gc.collect()\n        \n        #rmse_avg = np.mean(val_errors)\n        rmse_full = mean_squared_error(y_train, val_preds, squared=False)\n        \n    return rmse_full, val_preds, test_preds","metadata":{"execution":{"iopub.status.busy":"2024-09-23T15:02:17.556252Z","iopub.execute_input":"2024-09-23T15:02:17.556715Z","iopub.status.idle":"2024-09-23T15:02:17.570259Z","shell.execute_reply.started":"2024-09-23T15:02:17.556669Z","shell.execute_reply":"2024-09-23T15:02:17.568935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rmse_full_xgb, val_preds_xgb, test_preds_xgb = validation(XGBRegressor(**xgb_params))\n\nrmse_full_lgbm, val_preds_lgbm, test_preds_lgbm = validation(LGBMRegressor(**lgbm_params))\n\nrmse_full_cat, val_preds_cat, test_preds_cat = validation(CatBoostRegressor(**cat_params))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T15:02:22.639841Z","iopub.execute_input":"2024-09-23T15:02:22.640409Z","iopub.status.idle":"2024-09-23T15:04:19.874158Z","shell.execute_reply.started":"2024-09-23T15:02:22.640334Z","shell.execute_reply":"2024-09-23T15:04:19.872848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 4)\nsns.boxplot(data=y_train,ax=axes[0]).set_title('y-train')\nsns.boxplot(data=val_preds_xgb,ax=axes[1]).set_title('val_preds_xgb')\nsns.boxplot(data=val_preds_lgbm,ax=axes[2]).set_title('val_preds_lgbm')\nsns.boxplot(data=val_preds_cat,ax=axes[3]).set_title('val_preds_cat')\nfig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:24:58.963577Z","iopub.execute_input":"2024-09-18T14:24:58.964211Z","iopub.status.idle":"2024-09-18T14:24:59.999638Z","shell.execute_reply.started":"2024-09-18T14:24:58.964168Z","shell.execute_reply":"2024-09-18T14:24:59.998715Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" All three models underpredict vehicle prices for most training samples.","metadata":{}},{"cell_type":"code","source":"plt.scatter(y_train, val_preds_xgb, alpha=0.5)\nplt.title('Actual vs Predicted Values')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:25:00.000757Z","iopub.execute_input":"2024-09-18T14:25:00.001068Z","iopub.status.idle":"2024-09-18T14:25:01.136175Z","shell.execute_reply.started":"2024-09-18T14:25:00.001034Z","shell.execute_reply":"2024-09-18T14:25:01.135222Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(y_train, val_preds_lgbm, alpha=0.5)\nplt.title('Actual vs Predicted Values')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:25:01.138035Z","iopub.execute_input":"2024-09-18T14:25:01.138372Z","iopub.status.idle":"2024-09-18T14:25:02.219202Z","shell.execute_reply.started":"2024-09-18T14:25:01.138338Z","shell.execute_reply":"2024-09-18T14:25:02.218455Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(y_train, val_preds_cat, alpha=0.5)\nplt.title('Actual vs Predicted Values')\nplt.xlabel('Actual Values')\nplt.ylabel('Predicted Values')","metadata":{"execution":{"iopub.status.busy":"2024-09-18T14:25:02.220295Z","iopub.execute_input":"2024-09-18T14:25:02.220626Z","iopub.status.idle":"2024-09-18T14:25:03.385344Z","shell.execute_reply.started":"2024-09-18T14:25:02.220592Z","shell.execute_reply":"2024-09-18T14:25:03.384430Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 8. Optuna Weights and Submission.","metadata":{}},{"cell_type":"markdown","source":"Optuna is used to find the optimal weights for an ensemble of three models. The weights that minimize full train dataset's rmse score are needed.","metadata":{}},{"cell_type":"code","source":"class OptunaWeights:\n    def __init__(self, random_state, n_trials=5000):\n        self.study = None\n        self.weights = None\n        self.random_state = random_state\n        self.n_trials = n_trials\n\n    def _objective(self, trial, y_true, y_preds):\n        # Define the weights for the predictions from each model\n        weights = [trial.suggest_float(f\"weight{n}\", 0, 1) for n in range(len(y_preds) - 1)]\n        weights.append(1 - sum(weights))  # Ensure the sum of weights is 1\n\n        # Calculate the weighted prediction\n        weighted_pred = np.average(np.array(y_preds), axis=0, weights=weights)\n\n        rmse_full = mean_squared_error(y_true, weighted_pred, squared=False)\n        return rmse_full  \n\n    def fit(self, y_true, y_preds):\n        optuna.logging.set_verbosity(optuna.logging.ERROR)\n        sampler = CmaEsSampler(seed=self.random_state)\n        pruner = HyperbandPruner()\n        self.study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\",\n                                         direction='minimize')\n        objective_partial = partial(self._objective, y_true=y_true, y_preds=y_preds)\n        self.study.optimize(objective_partial, n_trials=self.n_trials, show_progress_bar=True)\n        weights = [self.study.best_params[f\"weight{n}\"] for n in range(len(y_preds) - 1)]\n        weights.append(1 - sum(weights))  # Ensure the sum of weights is 1\n        self.weights = weights","metadata":{"execution":{"iopub.status.busy":"2024-09-23T15:08:47.387513Z","iopub.execute_input":"2024-09-23T15:08:47.387942Z","iopub.status.idle":"2024-09-23T15:08:47.405374Z","shell.execute_reply.started":"2024-09-23T15:08:47.387895Z","shell.execute_reply":"2024-09-23T15:08:47.404274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ow = OptunaWeights(random_seed)\now.fit(y_train, y_preds=[val_preds_xgb, val_preds_lgbm, val_preds_cat])\nweights = ow.weights\nprint(weights)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T15:08:48.215187Z","iopub.execute_input":"2024-09-23T15:08:48.216055Z","iopub.status.idle":"2024-09-23T15:10:15.777898Z","shell.execute_reply.started":"2024-09-23T15:08:48.215982Z","shell.execute_reply":"2024-09-23T15:10:15.776681Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = weights[0] * test_preds_xgb + weights[1] * test_preds_lgbm + weights[2] * test_preds_cat\nsample_submission = pd.read_csv('/kaggle/input/playground-series-s4e9/sample_submission.csv')\nsample_submission['price'] = y_test\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2024-09-23T15:10:23.245598Z","iopub.execute_input":"2024-09-23T15:10:23.246021Z","iopub.status.idle":"2024-09-23T15:10:23.733351Z","shell.execute_reply.started":"2024-09-23T15:10:23.245973Z","shell.execute_reply":"2024-09-23T15:10:23.732084Z"},"trusted":true},"execution_count":null,"outputs":[]}]}